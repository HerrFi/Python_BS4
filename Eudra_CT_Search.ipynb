{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvT2AWHZyDSh",
        "outputId": "f6ec556c-a1c3-4b55-cc02-770ce01679cb"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import datetime\n",
        "\n",
        "#indications = {0: 'Borderline Personality Disorder', 1: 'Major Depressive Disorder', 2: 'ADHD', 3: 'Bipolar disorder' , 4: 'Substance abuse' , 5: 'PTSD', 6: 'Adjustment disorder' , 7: 'Generalised Anxiety Disorder' , 8: 'Conduct Disorder' , 9: 'Panic Disorder' , 10: 'Schizophrenia' , 11: 'Tourette', 12: 'Resistant Depression', 13: 'Autism', 14: 'Alzheimer'}\n",
        "indications = pd.read_csv('./Trial.csv')\n",
        "#print(indications)\n",
        "\n",
        "def indication_search(indication, options={}, verbose=False):\n",
        "    \"\"\"\n",
        "    Given the name of an indication, search the EU Clinical Trials Register and save the results as a DataFrame\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(indication)\n",
        "        if 'page' in options:\n",
        "            print(f'Page: {options[\"page\"]}')\n",
        "        else:\n",
        "            print('No page specified')\n",
        "    # Create search URL\n",
        "    print(indication)\n",
        "\n",
        "    indication_string = indication.replace(' ', '+')\n",
        "    search_url = f'https://www.clinicaltrialsregister.eu/ctr-search/search?query={indication_string}'\n",
        "    \n",
        "    # retrieve results from the database as an HTML page\n",
        "    with warnings.catch_warnings(record=True) as cw:\n",
        "        results_page = requests.get(search_url, verify=False, params=options)\n",
        "    final_url = results_page.url\n",
        "    if verbose:\n",
        "        print(final_url)\n",
        "    \n",
        "    # parse the HTML structure using Beautiful Soup\n",
        "    soup = BeautifulSoup(results_page.text)\n",
        "    \n",
        "    # find the number of hits\n",
        "    hit_count_string = soup.find('a', id='ui-id-1').text.strip()\n",
        "    n_hits = int(re.search('\\(([0-9]+)\\)', hit_count_string).group(1))\n",
        "    # print(n_hits)\n",
        "    \n",
        "    if n_hits > 0:\n",
        "        # get number of hits, pages, and current page\n",
        "        pages_string = soup.find('div', 'outcome grid_12').text.strip()\n",
        "        sub_strings = pages_string.split()\n",
        "\n",
        "        hits = int(sub_strings[0])\n",
        "        current_page = int(sub_strings[-3])\n",
        "        n_pages = int(sub_strings[-1].strip('.'))\n",
        "        if verbose:\n",
        "            print(f'{hits} hits; page {current_page}/{n_pages}')\n",
        "\n",
        "        # check that we have found the correct values\n",
        "        assert hits == n_hits\n",
        "\n",
        "        if 'page' in options:\n",
        "            assert current_page == int(options['page'])\n",
        "\n",
        "        # find all tables of class 'result' - one per trial found\n",
        "        tables = soup.find_all('table', 'result')\n",
        "\n",
        "        table_list = []\n",
        "\n",
        "        # loop through results\n",
        "        for current_table in tables:\n",
        "            # find table cells\n",
        "            data_cells = current_table.find_all('td')\n",
        "            cell_dict = {}\n",
        "            # loop through the cells\n",
        "            for cell in data_cells:\n",
        "                # does the cell contain a 'label' <span> tag?\n",
        "                label = cell.find('span', 'label')\n",
        "                if label:\n",
        "                    cell_text = cell.text\n",
        "                    # split cell text on first colon\n",
        "                    parts = cell.text.split(':', maxsplit=1)\n",
        "                    # clean up label and value strings\n",
        "                    label_str = parts[0].strip()\n",
        "                    value_str = parts[1].strip()\n",
        "                    # save in a dictionary\n",
        "                    cell_dict[label_str] = value_str\n",
        "            # save the dictonary for each cell in a list\n",
        "            table_list.append(cell_dict)\n",
        "        # convert the list of dictionaries into a DataFrame\n",
        "        hit_frame = pd.DataFrame(table_list)\n",
        "\n",
        "        if current_page < n_pages:  # need to get subsequent page(s)\n",
        "            new_options = dict(options)\n",
        "            new_options['page'] = f'{current_page + 1}'\n",
        "            sub_frame = indication_search(indication, options=new_options)\n",
        "            combined_frame = hit_frame.append(sub_frame, ignore_index=True)\n",
        "        else:\n",
        "            combined_frame = hit_frame\n",
        "\n",
        "        if 'Disease' in combined_frame.columns:\n",
        "            combined_frame.drop('Disease', axis=1, inplace=True)\n",
        "\n",
        "        if current_page == 1:\n",
        "            rows, cols = combined_frame.shape\n",
        "            print(f'{indication}: {rows} of {n_hits} records retrieved from {n_pages} pages')\n",
        "            assert rows == n_hits\n",
        "    else:\n",
        "        print(f'{indication}: {n_hits} found')\n",
        "        combined_frame = None\n",
        "    return combined_frame\n",
        "\n",
        "\n",
        "def result_link(in_frame):\n",
        "  results_available = in_frame['Trial results'].str.match('View results')\n",
        "  res_link = 'https://www.clinicaltrialsregister.eu/ctr-search/trial/' + in_frame[\"EudraCT Number\"][results_available] + '/results'\n",
        "  in_frame.loc[results_available, 'Trial results'] = res_link\n",
        "\n",
        "def protocol_link(row_input):\n",
        "  \"\"\"\n",
        "  Create protocol links for each country in each trial\n",
        "  \"\"\"\n",
        "  stem = 'https://www.clinicaltrialsregister.eu/ctr-search/trial/'\n",
        "  links = []\n",
        "  for ccode in row_input.iloc[0:-2]:\n",
        "      if type(ccode) is str:\n",
        "          links.append(f'{stem}{row_input.iloc[-1]}/{ccode}')\n",
        "      else:\n",
        "          links.append('')\n",
        "  return pd.Series(links, dtype=str)\n",
        "\n",
        "def country_links(in_frame):\n",
        "  countries = in_frame['Trial protocol'].str.extractall(r'([A-Z]{2})\\n')\n",
        "  countries = countries.unstack(level=-1)\n",
        "  countries.columns = countries.columns.droplevel(0)\n",
        "  new = countries.join(in_frame[\"EudraCT Number\"])\n",
        "  link_frame = new.apply(protocol_link, axis=1)\n",
        "  return link_frame\n",
        "\n",
        "earliest_start = '2020-01-01'\n",
        "phase_names = {1: 'one', 2: 'two', 3: 'three', 4: 'four'}\n",
        "\n",
        "search_options = {'dateFrom': earliest_start, 'phase': ['phase-' + name for name in phase_names.values()]}\n",
        "option_frame = pd.DataFrame.from_dict(search_options)\n",
        "\n",
        "frame_list = []\n",
        "for indication in indications.iterrows():\n",
        "    ind_frame = indication_search(indication[1]['Indication'], options=search_options)\n",
        "    if ind_frame is not None:\n",
        "        result_link(ind_frame)\n",
        "        link_frame = country_links(ind_frame)\n",
        "        new_frame = ind_frame.join(link_frame)    \n",
        "        frame_list.append(new_frame)\n",
        "    else:\n",
        "        frame_list.append(None)\n",
        "\n",
        "merged = pd.concat(frame_list, axis=0, keys=indications['Indication'])\n",
        "merged.head()\n",
        "\n",
        "merged.shape\n",
        "\n",
        "grouped = merged.groupby(['Sponsor Name', 'Indication'])\n",
        "\n",
        "summary = grouped.count()\n",
        "all_columns = list(summary.columns)\n",
        "summary.drop(all_columns[1:], axis=1, inplace=True)\n",
        "summary.rename(columns={'EudraCT Number': 'Count'})\n",
        "summary\n",
        "\n",
        "indication_counts = merged.groupby('Indication').count()['EudraCT Number']\n",
        "indication_counts\n",
        "\n",
        "sponsor_counts = merged.groupby('Sponsor Name').count()['EudraCT Number']\n",
        "sorted_counts = sponsor_counts.sort_values(ascending=False)\n",
        "pd.DataFrame(sorted_counts).head(14)\n",
        "\n",
        "\n",
        "now = datetime.datetime.utcnow()\n",
        "stamp = now.strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "\n",
        "xlname = f'EudraCTsearch_{stamp}.xlsx'\n",
        "print(xlname)\n",
        "with pd.ExcelWriter(xlname) as xlwriter:\n",
        "    option_frame.to_excel(xlwriter, sheet_name='Search options')\n",
        "    merged.to_excel(xlwriter, sheet_name='Merged')\n",
        "    summary.to_excel(xlwriter, sheet_name='Summary')\n",
        "    indication_counts.to_excel(xlwriter, sheet_name='Indications')\n",
        "    sorted_counts.to_excel(xlwriter, sheet_name='Sponsors')\n",
        "    for ind_series, frame in zip(indications.iterrows(), frame_list):\n",
        "        if frame is not None:\n",
        "            frame.to_excel(xlwriter, sheet_name=ind_series[1]['Indication'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Borderline Personality Disorder\n",
            "Borderline Personality Disorder: 3 of 3 records retrieved from 1 pages\n",
            "Major Depressive Disorder\n",
            "Major Depressive Disorder\n",
            "Major Depressive Disorder\n",
            "Major Depressive Disorder: 52 of 52 records retrieved from 3 pages\n",
            "ADHD\n",
            "ADHD\n",
            "ADHD: 22 of 22 records retrieved from 2 pages\n",
            "Bipolar disorder\n",
            "Bipolar disorder\n",
            "Bipolar disorder: 25 of 25 records retrieved from 2 pages\n",
            "Substance abuse\n",
            "Substance abuse: 8 of 8 records retrieved from 1 pages\n",
            "PTSD\n",
            "PTSD: 13 of 13 records retrieved from 1 pages\n",
            "Adjustment disorder\n",
            "Adjustment disorder: 18 of 18 records retrieved from 1 pages\n",
            "Generalised Anxiety Disorder\n",
            "Generalised Anxiety Disorder: 1 of 1 records retrieved from 1 pages\n",
            "Conduct Disorder\n",
            "Conduct Disorder\n",
            "Conduct Disorder: 25 of 25 records retrieved from 2 pages\n",
            "Panic Disorder\n",
            "Panic Disorder: 0 found\n",
            "Schizophrenia\n",
            "Schizophrenia\n",
            "Schizophrenia\n",
            "Schizophrenia: 51 of 51 records retrieved from 3 pages\n",
            "EudraCTsearch_20220826_201426.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WwS4pUmgvtxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YmnZg24IksJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "J6ZgU9WhqnLZ"
      }
    }
  ]
}